{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf44680-b0f2-4ad4-8747-a0af7ef0ce46",
   "metadata": {},
   "source": [
    "## Importing MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e9b6cc-a4bf-495c-9cb6-c7e412a19cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is a sample Notebook to demonstrate how to read \"MNIST Dataset\"\n",
    "#\n",
    "import numpy as np # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())\n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33095f8-bf2b-40eb-aba2-2b7185088699",
   "metadata": {},
   "source": [
    "### Verifiying Dataset Was Loaded Correctly via MnistDataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa06b04-37d7-417d-86a0-70c89094e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "input_path = 'MNIST'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte') #60,000 training images\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte') #10,000 test images\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "print(training_images_filepath)\n",
    "print(training_labels_filepath)\n",
    "print(test_images_filepath)\n",
    "print(test_labels_filepath)\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray) # values go from 0 (black) to 256\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "\n",
    "#\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcbd486-7544-46ac-bb97-9614625c0c82",
   "metadata": {},
   "source": [
    "### Showing Some of the Images Based on Selected File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e04653b-07a1-4a10-a612-4e8b95867e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Show some random training and test images \n",
    "#\n",
    "images_2_show = []\n",
    "titles_2_show = []\n",
    "for i in range(0, 1): # how many random training images you want to show\n",
    "    r = random.randint(1, 60000)\n",
    "    images_2_show.append(x_train[r])\n",
    "    titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train[r]))    \n",
    "\n",
    "for i in range(0, 1): # how many random test images you want to show\n",
    "    r = random.randint(1, 10000)\n",
    "    images_2_show.append(x_test[r])        \n",
    "    titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))    \n",
    "\n",
    "show_images(images_2_show, titles_2_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc368a3a-62a2-4852-b056-d672a49bb01f",
   "metadata": {},
   "source": [
    "## Creating Neural Network (one hidden layer)\n",
    "\n",
    "There will be 2 layers, excluding the inputs. The inputs will be 28 * 28 neurons or 784 in total, the hidden layer will have 300 neurons, and then the output will have 10 neurons representing the likihood of each digit in this classification problem after applying the softmax function to the logit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070804f-b2e1-4000-87ae-79ace4b87f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructing mini-batch to be input (reused function to get test inputs as well)\n",
    "def GetMiniBatch(x_train, y_train, numInputs): \n",
    "    miniBatch = np.zeros((784,numInputs))\n",
    "    y_miniBatch = np.zeros((10,numInputs))\n",
    "    for i in range(numInputs):\n",
    "        r = random.randint(0, len(x_train) - 1) #we may sample the same images again from another batch but this will be fixed later on\n",
    "        temp = np.array(x_train[r])\n",
    "        y_miniBatch[y_train[r],i] = 1 #putting a 1 where the correct label is \n",
    "        temp = np.reshape(temp, 784) / 255 #dividing by 255 to change range to [0,1] to prevent overflowing when computing softmax later on\n",
    "        miniBatch[:,i] = temp\n",
    "    return miniBatch, y_miniBatch  #each column represents a sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af52150f-9f16-4075-b930-4f3a2753935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Checking first layer if values are correct\n",
    "temp = np.matmul(W1, miniBatch)\n",
    "z1 = temp + b1 # (300 X 784) X (784 X 100) + (300 X 1) => (300 X 100)\n",
    "for j in range(10):\n",
    "    print(\"Checking that temp + b1 is correct for index {}, temp:{}, temp + b1: {}, z1: {}\".format(j, temp[0][j], temp[0][j] + b1[0], z1[0][j]))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e03ae6f-c6e0-4695-a931-6ad57a5a6f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makeshift Gradient Checker used for Debugging where I changed one value in one of the parameters to find derivative of loss w.r.t to that value\n",
    "\n",
    "# def GradientChecker(miniBatch, y_miniBatch, p, e):\n",
    "#     W2 = p[0]\n",
    "#     b2 = p[1]\n",
    "#     W1 = p[2]\n",
    "#     b1 = p[3]\n",
    "    \n",
    "#     #J(W2 + e)\n",
    "#     #First Layer\n",
    "#     tempW1 = np.copy(W1)\n",
    "#     tempW1[200,200] += e\n",
    "#     z1 = np.matmul(tempW1, miniBatch) + b1 # (300 X 784) X (784 X 100) + (300 X 1) => (300 X 100)\n",
    "#     a1 = sigmoid(z1)\n",
    "#     #Second layer\n",
    "#     z2 = np.matmul(W2, a1) + b2 # LOGIT: (10 X 300) X (300 X 100) + (10 X 1) => (10 X 100) \n",
    "#     #Softmax\n",
    "#     rows, cols = z2.shape\n",
    "#     softmax_denominator = np.zeros((rows,cols))\n",
    "#     for i in range(cols):\n",
    "#         denominator_sum = np.sum(np.exp(z2[:,i]))\n",
    "#         softmax_denominator[:,i] = np.full(rows, denominator_sum)\n",
    "        \n",
    "#     softmax = np.exp(z2) / softmax_denominator #final output normalized as probabilities\n",
    "#     loss_1 = Loss(softmax, y_miniBatch)\n",
    "    \n",
    "#     #J(W2 - e)\n",
    "#     #First Layer\n",
    "#     tempW1 = np.copy(W1)\n",
    "#     tempW1[200,200] -= e\n",
    "#     z1 = np.matmul(W1, miniBatch) + b1 # (300 X 784) X (784 X 100) + (300 X 1) => (300 X 100)\n",
    "#     a1 = sigmoid(z1)\n",
    "#     #Second layer\n",
    "#     z2 = np.matmul(W2, a1) + b2 # LOGIT: (10 X 300) X (300 X 100) + (10 X 1) => (10 X 100) \n",
    "#     #Softmax\n",
    "#     rows, cols = z2.shape\n",
    "#     softmax_denominator = np.zeros((rows,cols))\n",
    "#     for i in range(cols):\n",
    "#         denominator_sum = np.sum(np.exp(z2[:,i]))\n",
    "#         softmax_denominator[:,i] = np.full(rows, denominator_sum)\n",
    "#     softmax = np.exp(z2) / softmax_denominator #final output normalized as probabilities\n",
    "#     loss_2 = Loss(softmax, y_miniBatch)\n",
    "\n",
    "#     return (loss_1 - loss_2)/ (2 * e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cde00e-3877-4dac-9761-99dad0481876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Entropy Loss for specified minibatch, TODO: change implementation to just mulitply element wise for efficiency\n",
    "def Loss(softmax, y_miniBatch):\n",
    "    # Softmax is (10 by 100) while y_miniBatch is (10 by 100) as well where y_miniBatch holds the one hot encoding\n",
    "    Loss_all_samples = -np.log(np.diag(np.matmul(y_miniBatch.T, softmax))) ## if the true probability distribution is a one hot encoding then formula reduces to -log(y_correct_label) or the negative log likelihood\n",
    "    return np.sum(Loss_all_samples) / y_miniBatch.shape[1] # loss over multiple samples is the average of all them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004fc88-80a3-4f93-9cab-57929fffacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(softmax, y_miniBatch):\n",
    "    correctClassifications = 0\n",
    "    numInputs = y_miniBatch.shape[1]\n",
    "    \n",
    "    maxiumums = np.max(softmax, axis=0)\n",
    "    for i in range(numInputs):\n",
    "        if np.dot(y_miniBatch[:,i], softmax[:,i]) == maxiumums[i]:\n",
    "            correctClassifications += 1\n",
    "            \n",
    "    return correctClassifications/numInputs\n",
    "\n",
    "#Accuracy(np.array([[.2,.4],[.8,.6]]), np.array([[1,0],[0,1]])) -- testing accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105c506-64b0-4bea-b544-fd97eb77b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation Functions\n",
    "def sigmoid(z):\n",
    "    return (1/(1 + np.exp(-z)))\n",
    "            \n",
    "def sigmoidDeriv(z):\n",
    "    return (1 - sigmoid(z))\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def ReLUDeriv(z):\n",
    "    boolMask = z > 0\n",
    "    return boolMask.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee6a38-fdfc-4a12-9be6-97adc44b0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Foward  and Backward Propagation (assuming 100 sample inputs)\n",
    "def ForwardAndBackwardProp(miniBatch, y_miniBatch, p, learning_rate, layers):\n",
    "    W2 = p[0]\n",
    "    b2 = p[1]\n",
    "    W1 = p[2]\n",
    "    b1 = p[3]\n",
    "    \n",
    "    #First Layer\n",
    "    z1 = np.matmul(W1, miniBatch) + b1\n",
    "    a1 = ReLU(z1)\n",
    "    #Second layer\n",
    "    z2 = np.matmul(W2, a1) + b2 # LOGIT\n",
    "    #Softmax\n",
    "    rows, cols = z2.shape\n",
    "    softmax_denominator = np.zeros((rows,cols))\n",
    "    for i in range(cols):\n",
    "        denominator_sum = np.sum(np.exp(z2[:,i]))\n",
    "        softmax_denominator[:,i] = np.full(rows, denominator_sum)\n",
    "\n",
    "    softmax = np.exp(z2) / softmax_denominator #final output normalized as probabilities\n",
    "    #print(\"Sum of a column in softmax: {}\".format(np.sum(softmax[:,2]))) #checking a column of the final output equals 1\n",
    "    #print(softmax[:,0])\n",
    "    # Backward Prop (Calculating Gradients)\n",
    "    num_inputs = miniBatch.shape[1]\n",
    "    localError_l2 = softmax - y_miniBatch # derivative of the cross entropy loss with respect to z2 assuming only a softmax is done on z2 is the p - y where is p is the logit and y is the one hot vetor for this classification task \n",
    "    \n",
    "    dW2 = np.zeros((layers[2], layers[1]))\n",
    "    db2 = np.zeros((layers[2], 1))\n",
    "    db1 = np.zeros((layers[1], 1))\n",
    "    dW1 = np.zeros((layers[1], layers[0]))\n",
    "    \n",
    "    db2 = (np.sum(localError_l2, axis=1)).reshape((10,1)) #Adding up the localerror along each row\n",
    "    dW2 = np.matmul(localError_l2, a1.T)\n",
    "    \n",
    "    localError_l1 = np.multiply(np.matmul(W2.T, localError_l2), ReLUDeriv(z1))\n",
    "    db1 = (np.sum(localError_l1, axis=1)).reshape((300,1))\n",
    "    dW1 = np.matmul(localError_l1, miniBatch.T) \n",
    "    #Checking Gradient\n",
    "    #dW1_verified_01 = GradientChecker(miniBatch, y_miniBatch, p, 0.0001)\n",
    "    #print(\"This is verified: {}, this is acutal: {}\".format(dW1_verified_01, dW1[200,200]/num_inputs))\n",
    "    \n",
    "    #Updating Parameters\n",
    "    derivParam = [dW2, db2, dW1, db1]\n",
    "    pNew = []\n",
    "    \n",
    "    for i in range(len(p)):\n",
    "        derivParam[i] /= num_inputs     #Since we take an average of the sum of each sample's loss to get overall loss we must divide by number of inputs\n",
    "        pNew.append(p[i] - (learning_rate * derivParam[i]))\n",
    "    return pNew, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b80e9-984e-49a7-9b0b-bf0435182445",
   "metadata": {},
   "source": [
    "### Time to train! Putting it all together :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f8499-5544-42b7-9a30-eb574e639c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters in neural network\n",
    "layers = [784, 300, 10]\n",
    "neurons_input, neurons_l1, neurons_output = layers[0], layers[1], layers[2]\n",
    "randomGen = np.random.default_rng(seed=42)\n",
    "numInputs = 60\n",
    "\n",
    "#Initialization matters! -- results worse with this initialization\n",
    "# W2 = randomGen.random((neurons_output, neurons_l1)) # 10 rows, 300 columns\n",
    "# b2 = randomGen.random((neurons_output, 1))\n",
    "# W1 = randomGen.random((neurons_l1, neurons_input)) #300 rows, 784 columns\n",
    "# b1 = randomGen.random((neurons_l1, 1))\n",
    "\n",
    "W2 = randomGen.uniform(low = -(1/np.sqrt(numInputs)), high = (1/np.sqrt(numInputs)), size = (neurons_output, neurons_l1)) # 10 rows, 300 columns\n",
    "b2 = randomGen.uniform(low = -(1/np.sqrt(numInputs)), high = (1/np.sqrt(numInputs)), size = (neurons_output, 1))\n",
    "W1 = randomGen.uniform(low = -(1/np.sqrt(numInputs)), high = (1/np.sqrt(numInputs)), size = (neurons_l1, neurons_input)) #300 rows, 784 columns\n",
    "b1 = randomGen.uniform(low = -(1/np.sqrt(numInputs)), high = (1/np.sqrt(numInputs)), size = (neurons_l1, 1))\n",
    "\n",
    "p = [W2, b2, W1, b1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ab152-6443-49a7-bd65-64a5cc01b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "epochs = 1000\n",
    "lossOverTime = []\n",
    "accuracyOverTime = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    miniBatch, y_miniBatch = GetMiniBatch(x_train, y_train, numInputs)\n",
    "    pNew, softmax = ForwardAndBackwardProp(miniBatch, y_miniBatch, p, 0.01, [784,300,10])\n",
    "    tempLoss = Loss(softmax, y_miniBatch)\n",
    "    tempAccuracy = Accuracy(softmax, y_miniBatch)\n",
    "    lossOverTime.append(tempLoss)\n",
    "    accuracyOverTime.append(tempAccuracy)\n",
    "    p = pNew\n",
    "    \n",
    "plt.plot(lossOverTime)\n",
    "plt.title(\"lossOverTime\")\n",
    "plt.show()\n",
    "plt.plot(accuracyOverTime)\n",
    "plt.title(\"accuracyOverTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241609cb-ebb9-4075-9722-1a7bc57c7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "testInputs = 10000\n",
    "test_miniBatch, test_y_miniBatch = GetMiniBatch(x_test, y_test, testInputs)\n",
    "pNew, softmax = ForwardAndBackwardProp(test_miniBatch, test_y_miniBatch, p, 0.01, [784,300,10])\n",
    "print(\"Loss for {} inputs: {}\".format(testInputs, Loss(softmax, test_y_miniBatch)))\n",
    "print(\"Accuracy for {} inputs: {}\".format(testInputs, Accuracy(softmax, test_y_miniBatch)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ef926-a1d9-470e-9301-fb75e0f2c921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
